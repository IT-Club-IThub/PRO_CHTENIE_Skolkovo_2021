{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('..')\n",
    "\n",
    "import json\n",
    "import numpy as np # Для работы с данными \n",
    "import sys\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer # Методы для работы с текстами\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "JSON_DATA_DIR = '../../Satellites'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# создаём два списка, в пустые элементы которых будем добавлять тексты\n",
    "# в первом элементе будет храниться текст с ответом False, во втором - True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSetFromIndexes(wordIndexes, xLen, step):\n",
    "  xText = []\n",
    "  wordsLen = len(wordIndexes) # Считаем количество слов\n",
    "  index = 0 # Задаем начальный индекс \n",
    "\n",
    "  while (index + xLen <= wordsLen): # Идём по всей длине вектора индексов\n",
    "    xText.append(wordIndexes[index:index+xLen]) # \"Откусываем\" векторы длины xLen\n",
    "    index += step # Смещаемся вперёд на step\n",
    "    \n",
    "  return xText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createSetsMultiClassesTest(wordIndexes, xLen, step): # Функция принимает последовательность индексов, размер окна, шаг окна\n",
    "  classesXSamples = []        # Здесь будет список размером \"кол-во классов*кол-во окон в тексте*длину окна\"\n",
    "  for wI in wordIndexes:      # Для каждого текста выборки из последовательности индексов\n",
    "    classesXSamples.append(getSetFromIndexes(wordIndexes[0], xLen, step))\n",
    "\n",
    "\n",
    "  xSamples = np.array(classesXSamples[0])\n",
    "\n",
    "  return xSamples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_xTest(elem):\n",
    "    textClasses = ['', '']\n",
    "\n",
    "    # открываем файл с id эссе и ответами\n",
    "    with open(JSON_DATA_DIR + '/train/train_standart.json', 'r') as f_list:\n",
    "        data = json.load(f_list)\n",
    "\n",
    "    # проходимся по каждому \"блоку\" с эссе\n",
    "    for i in range(len(data)):\n",
    "        elem = data[i]\n",
    "\n",
    "        with open(JSON_DATA_DIR + f'/train/essays/{elem[\"id\"]}.json', 'r') as essay:\n",
    "            file = json.load(essay)\n",
    "            text = file['text']\n",
    "            if elem['answer'] == False:\n",
    "                textClasses[0] += text\n",
    "                textClasses[0] += '#'\n",
    "            else:\n",
    "                textClasses[1] += text\n",
    "                textClasses[1] += '#'\n",
    "\n",
    "    texts_false = textClasses[0].split(\"#\")\n",
    "    texts_true = textClasses[1].split(\"#\")\n",
    "\n",
    "    trainText = []\n",
    "    trainText.append(' '.join(texts_false))\n",
    "    trainText.append(' '.join(texts_true))\n",
    "\n",
    "    tokenizer = Tokenizer(\n",
    "                filters='!\"#$%&()*+,-–—./…:;<=>?@[\\\\]^_`{|}~«»\\t\\n\\xa0\\ufeff',\n",
    "                lower=True,\n",
    "                split=' ',\n",
    "                oov_token='unknown',\n",
    "                char_level=False)\n",
    "\n",
    "    tokenizer.fit_on_texts(trainText)   \n",
    "\n",
    "\n",
    "    testText = []\n",
    "\n",
    "    with open(JSON_DATA_DIR + f'/test/essays/{elem[\"id\"]}.json', 'r') as essay:\n",
    "        file = json.load(essay)\n",
    "        text = file['text']\n",
    "        testText.append(text)\n",
    "\n",
    "    xLen = 300\n",
    "\n",
    "    testTextArray = tokenizer.texts_to_sequences(testText[i][0])\n",
    "    xTest = createSetsMultiClassesTest(testTextArray, xLen, xLen)\n",
    "\n",
    "    return xTest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
