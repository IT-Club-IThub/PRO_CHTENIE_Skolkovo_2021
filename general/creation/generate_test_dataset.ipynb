{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('..')\n",
    "from config import Config\n",
    "\n",
    "import json\n",
    "import numpy as np # Для работы с данными \n",
    "import sys\n",
    "import os\n",
    "\n",
    "from tensorflow.keras import utils # Для работы с категориальными данными\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer # Методы для работы с текстами"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "JSON_DATA_DIR = '../../Satellites'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Считаю, почти все дадут согласие, что в протяжении жизни мы ни один раз встречаем безответственных профессионалов собственного дела. К несчастью, если дело касается медицины, то безалаберное отношение может сыграть злую шуточку. Над неувязкой ответственности анализирует автор текста. Василий Павлович Аксенов думает, что к собственной работе нужно относится серьезно и радиво, в особенности если идет речь о здоровье остальных людей. Свое мнение автор обосновывает на примере ситуации из текста.\\nВасилий Павлович Аксенов думает, что к собственной работе нужно относится серьезно и радиво, в особенности если идет речь о здоровье остальных людей. Свое мнение автор обосновывает на примере ситуации из текста. Василий Павлович противопоставляет 2-ух мед сотрудников различных поколений. Юный спец Александр Зеленин чрезвычайно трепетно подступает к реализации собственных обязательств. Он не может «закрыть глаза» на почти все упущения. Автор подчеркивает это в предложениях 2,3. Не считая того свежеиспеченный руководитель не лишь проходится по поликлинике, да и изучает старые истории заболеваний. Это подтверждается в предложениях 11-12. В целях изменений ответственный Зеленин собирает производственное заседание. Юному спецу автор противопоставляет медработника Макара Ивановича со стажем работы больше 30 лет. Скорее всего, мужчине уже надоела своя работа и он стал безответственно подступать к её реализации. Автор подчеркивает это в предложениях 9-12, 18. Не считая того Макар Иванович не ощущает, что делает что-то не так, он не желает признавать собственных просчетов и поменять подход к исцелению пациентов. Из предложений 36-47 мы можем прийти к выводу, что медработник не хочет никаких изменений и, при этом, не намерена прислушиваться к словам юного руководителя. Позиция автора ясна. К хоть какой работе нужно относится радиво, в особенности если от нее зависит здоровье остальных людей. В наше время повсевременно что-то изменяется, возникает. Конкретно потому нельзя отставать от развития. Я всецело согласна с позицией автора. Считаю, во всегда были люди, безответственно пригодные к собственной работе. К примеру, в романе Ивана Александровича Гончарова «Обломов» основной герой Илья Ильич привык к бесцельному стилю жизни, он запамятывает об управлении своим имением и всецело теряет связь с работой. Не умопомрачительно, что с каждым годом жизнь в Обломовке становится все ужаснее и ужаснее. В итоге, подводя результаты произнесенному, можно прийти к выводу. Любой из нас должен с совестью подступать к выполнению собственных должностных обязательств. Нужно осознавать, что все взаимосвязано, и от нашей ответственности зависит благополучие остальных людей.'], False]\n"
     ]
    }
   ],
   "source": [
    "# создаём два списка, в пустые элементы которых будем добавлять тексты\n",
    "# в первом элементе будет храниться текст с ответом False, во втором - True\n",
    "textClasses = ['', '']\n",
    "testTextClasses = []\n",
    "\n",
    "# открываем файл с id эссе и ответами\n",
    "with open(JSON_DATA_DIR + '/train/train_standart.json', 'r') as f_list:\n",
    "  data = json.load(f_list)\n",
    "\n",
    "  # проходимся по каждому \"блоку\" с эссе\n",
    "  for i in range(len(data)):\n",
    "    elem = data[i]\n",
    "    with open(JSON_DATA_DIR + f'/train/essays/{elem[\"id\"]}.json', 'r') as essay:\n",
    "      file = json.load(essay)\n",
    "      text = file['text']\n",
    "      if i < 114:\n",
    "        testTextClasses.append([[text], elem[\"answer\"]])\n",
    "        continue\n",
    "      if elem['answer'] == False:\n",
    "        textClasses[0] += text\n",
    "        textClasses[0] += '#'\n",
    "      else:\n",
    "        textClasses[1] += text\n",
    "        textClasses[1] += '#'\n",
    "\n",
    "print(testTextClasses[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSetFromIndexes(wordIndexes, xLen, step):\n",
    "  xText = []\n",
    "  wordsLen = len(wordIndexes) # Считаем количество слов\n",
    "  index = 0 # Задаем начальный индекс \n",
    "\n",
    "  while (index + xLen <= wordsLen): # Идём по всей длине вектора индексов\n",
    "    xText.append(wordIndexes[index:index+xLen]) # \"Откусываем\" векторы длины xLen\n",
    "    index += step # Смещаемся вперёд на step\n",
    "    \n",
    "  return xText\n",
    "\n",
    "def createSetsMultiClasses(wordIndexes, xLen, step): # Функция принимает последовательность индексов, размер окна, шаг окна\n",
    "  nClasses = len(wordIndexes) # Количество классов\n",
    "  classesXSamples = []        # Здесь будет список размером \"кол-во классов*кол-во окон в тексте*длину окна\"\n",
    "  for wIclass in range(nClasses):      # Для каждого текста выборки из последовательности индексов\n",
    "    if wIclass == 0:\n",
    "      classesXSamples.append(getSetFromIndexes(wordIndexes[0], xLen, step[0]))\n",
    "    else:\n",
    "      classesXSamples.append(getSetFromIndexes(wordIndexes[1], xLen, step[1]))\n",
    "\n",
    "  # Формируем один общий xSamples\n",
    "  xSamples = []\n",
    "  ySamples = []\n",
    "  \n",
    "  for t in range(nClasses):\n",
    "    xT = classesXSamples[t]\n",
    "    for i in range(len(xT)): # Перебираем каждое окно определенного класса\n",
    "      xSamples.append(xT[i]) # Добавляем в общий список выборки\n",
    "      ySamples.append(utils.to_categorical(t, nClasses)) # Добавляем соответствующий вектор класса\n",
    "\n",
    "  xSamples = np.array(xSamples)\n",
    "  ySamples = np.array(ySamples)\n",
    "\n",
    "  print(xSamples.shape)\n",
    "\n",
    "  return (xSamples, ySamples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSetFromIndexes(wordIndexes, xLen, step):\n",
    "  xText = []\n",
    "  wordsLen = len(wordIndexes) # Считаем количество слов\n",
    "  index = 0 # Задаем начальный индекс \n",
    "\n",
    "  while (index + xLen <= wordsLen): # Идём по всей длине вектора индексов\n",
    "    xText.append(wordIndexes[index:index+xLen]) # \"Откусываем\" векторы длины xLen\n",
    "    index += step # Смещаемся вперёд на step\n",
    "    \n",
    "  return xText\n",
    "\n",
    "def createSetsMultiClassesTest(wordIndexes, xLen, step): # Функция принимает последовательность индексов, размер окна, шаг окна\n",
    "  nClasses = len(wordIndexes) # Количество классов\n",
    "  classesXSamples = []        # Здесь будет список размером \"кол-во классов*кол-во окон в тексте*длину окна\"\n",
    "  for wI in wordIndexes:      # Для каждого текста выборки из последовательности индексов\n",
    "    classesXSamples.append(getSetFromIndexes(wordIndexes[0], xLen, step))\n",
    "\n",
    "\n",
    "  # Формируем один общий xSamples\n",
    "  xSamples = []\n",
    "  ySamples = []\n",
    "  \n",
    "  for t in range(nClasses):\n",
    "    xT = classesXSamples[t]\n",
    "    for i in range(len(xT)): # Перебираем каждое окно определенного класса\n",
    "      xSamples.append(xT[i]) # Добавляем в общий список выборки\n",
    "      ySamples.append(utils.to_categorical(t, nClasses)) # Добавляем соответствующий вектор класса\n",
    "\n",
    "  xSamples = np.array(xSamples)\n",
    "  ySamples = np.array(ySamples)\n",
    "\n",
    "  return (xSamples, ySamples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['False', 'True']\n",
    "labelsNum = len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_false = textClasses[0].split(\"#\")\n",
    "texts_true = textClasses[1].split(\"#\")\n",
    "\n",
    "test_border_false = len(texts_false)//10\n",
    "test_border_true = len(texts_true)//10\n",
    "\n",
    "valText = []\n",
    "valText.append(' '.join(texts_false[:test_border_false]))\n",
    "valText.append(' '.join(texts_true[:test_border_true]))\n",
    "\n",
    "\n",
    "trainText = []\n",
    "trainText.append(' '.join(texts_false))\n",
    "trainText.append(' '.join(texts_true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49208\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(\n",
    "    filters='!\"#$%&()*+,-–—./…:;<=>?@[\\\\]^_`{|}~«»\\t\\n\\xa0\\ufeff',\n",
    "    lower=True,\n",
    "    split=' ',\n",
    "    oov_token='unknown',\n",
    "    char_level=False)\n",
    "\n",
    "tokenizer.fit_on_texts(trainText)\n",
    "\n",
    "print(len(list(tokenizer.word_index.items())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3737, 300)\n",
      "(402, 300)\n"
     ]
    }
   ],
   "source": [
    "numWords = 49208\n",
    "xLen = 300\n",
    "step = (150, 35)\n",
    "\n",
    "trainWordIndexes = tokenizer.texts_to_sequences(trainText)\n",
    "valWordIndexes = tokenizer.texts_to_sequences(valText)\n",
    "\n",
    "\n",
    "xTrainId, yTrain = createSetsMultiClasses(trainWordIndexes, xLen, step)\n",
    "xValId, yVal = createSetsMultiClasses(valWordIndexes, xLen, step)\n",
    "\n",
    "for i in range(len(testTextClasses)):\n",
    "    testTextArray = tokenizer.texts_to_sequences(testTextClasses[i][0])\n",
    "    xTestId, yTest = createSetsMultiClassesTest(testTextArray, xLen, xLen)\n",
    "    xTest = xTestId\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "np.savez_compressed(f\"{Config.DATASETS_DIR}NEWNEWdataset_numWords_{numWords}_xLen_{xLen}_step_{xLen}_.npz\", xTrain=xTrainId, yTrain=yTrain, xVal=xValId, yVal=yVal, xTest=xTest, yTest=yTest)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "17331542a1b186c80143aaa34f2da524bc3765ccabf78956ee6995701fe0dc3f"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
