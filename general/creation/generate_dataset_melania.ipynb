{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"QvmWuw8qjYaX"},"outputs":[],"source":["import json\n","import numpy as np # Для работы с данными \n","\n","from tensorflow.keras import utils # Для работы с категориальными данными\n","from tensorflow.keras.preprocessing.text import Tokenizer # Методы для работы с текстами"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"NE5t4uWtuIIR"},"outputs":[],"source":["dataset_dir = '../../Satellites'"]},{"cell_type":"markdown","metadata":{"id":"yEqPLQ4RWbUE"},"source":["Все тексты true и false объединены в два больших текста;\n","Датасет - список из двух элементов-строк"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":270},"executionInfo":{"elapsed":338,"status":"error","timestamp":1639498780212,"user":{"displayName":"Игорь Молчанов","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjYdLWH6k0781lHDY2fOHciQCMySUAFsaUrdGZFxQ=s64","userId":"05120680139566347989"},"user_tz":-180},"id":"UdQUzDqvvtat","outputId":"dbda315d-a548-4e59-efa6-f4b7def7ff43"},"outputs":[],"source":["# создаём два списка, в пустые элементы которых будем добавлять тексты\n","# в первом элементе будет храниться текст с ответом False, во втором - True\n","textClasses = ['', '']\n","\n","\n","# открываем файл с id эссе и ответами\n","with open(dataset_dir + '/train/train_standart.json', 'r') as f_list:\n","  data = json.load(f_list)\n","\n","  # проходимся по каждому \"блоку\" с эссе\n","  for i in range(len(data)):\n","    elem = data[i]\n","\n","    with open(dataset_dir + f'/train/essays/{elem[\"id\"]}.json', 'r') as essay:\n","      file = json.load(essay)\n","      text = file['text']\n","      if elem['answer'] == False:\n","        textClasses[0] += text\n","        textClasses[0] += '#'\n","      else:\n","        textClasses[1] += text\n","        textClasses[1] += '#'"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["texts_false = textClasses[0].split(\"#\")\n","texts_true = textClasses[1].split(\"#\")\n","border_false = len(texts_false)//10\n","border_true = len(texts_true)//10\n","\n","testText = []\n","testText.append(' '.join(texts_false[:border_false]))\n","testText.append(' '.join(texts_true[:border_true]))\n","\n","trainText = []\n","trainText.append(' '.join(texts_false[border_false:]))\n","trainText.append(' '.join(texts_true[border_true:]))"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"p36nUSOjV5cH"},"outputs":[],"source":["numWords = 5000 # Количество слов/индексов, которое мы будем учитывать при обучении\n","\n","# настраиваем токенайзер, все слова приводятся в нижний регистр, специальные символы опускаются\n","tokenizer = Tokenizer(num_words=numWords,\n","                      filters='!\"#$%&()*+,-–—./…:;<=>?@[\\\\]^_`{|}~«»\\t\\n\\xa0\\ufeff',\n","                      lower=True,\n","                      split=' ',\n","                      oov_token='unknown',\n","                      char_level=False)\n","\n","tokenizer.fit_on_texts(trainText) # словарь частотности\n","items = list(tokenizer.word_index.items()) # индексы слов"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"Y2OOrGFIYPLY"},"outputs":[{"name":"stdout","output_type":"stream","text":["Самые часто встречающиеся слова:\n","[('unknown', 1), ('в', 2), ('и', 3), ('на', 4), ('что', 5), ('не', 6), ('с', 7), ('к', 8), ('как', 9), ('а', 10)]\n","\n","Самые редко встречающиеся слова:\n","[('дозволено', 49642), ('мешавшей', 49643), ('старухину', 49644), ('жуткого', 49645), ('сломает', 49646), ('восставала', 49647), ('пришедший', 49648), ('пролитию', 49649), ('евангелие', 49650), ('несущей', 49651)]\n","\n","Размер словаря: 49651\n"]}],"source":["print(\"Самые часто встречающиеся слова:\")\n","print(items[:10])\n","print()\n","print(\"Самые редко встречающиеся слова:\")\n","print(items[-10:])\n","print()\n","print(\"Размер словаря:\", len(items))"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"xD1bhptVYRik"},"outputs":[],"source":["# Преобразовываем текст в последовательность индексов согласно частотному словарю\n","trainWordIndexes = tokenizer.texts_to_sequences(trainText)\n","testWordIndexes = tokenizer.texts_to_sequences(testText)"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"zOtbDxD0Ymg3"},"outputs":[],"source":["# классы, на которые делятся тексты\n","labels = ['false', 'true']\n","labelsNum = len(labels)"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"z3wSB0q4YfYl"},"outputs":[{"name":"stdout","output_type":"stream","text":["Статистика по обучающим текстам:\n","false       2339742 символов,  321705 слов\n","true        596130 символов,  82719 слов\n","----\n","В сумме  2935872  символов,  404424  слов \n","\n","\n","Статистика по тестовым текстам:\n","false       247810 символов,  35057 слов\n","true        58625 символов,  8397 слов\n","----\n","В сумме  306435  символов,  43454  слов\n"]}],"source":["print(\"Статистика по обучающим текстам:\")\n","\n","symbolsTrainText = 0\n","wordsTrainText = 0\n","\n","for i in range(labelsNum):\n","  print(labels[i], \" \"*(10-len(labels[i])), len(trainText[i]), \"символов, \", len(trainWordIndexes[i]), \"слов\")\n","  symbolsTrainText += len(trainText[i])\n","  wordsTrainText += len(trainWordIndexes[i])\n","\n","print('----')\n","print(\"В сумме \", symbolsTrainText, \" символов, \", wordsTrainText, \" слов \\n\")\n","print()\n","print(\"Статистика по тестовым текстам:\")\n","\n","symbolsTestText = 0\n","wordsTestText = 0\n","\n","for i in range(labelsNum):\n","  print(labels[i], ' '*(10-len(labels[i])), len(testText[i]), \"символов, \", len(testWordIndexes[i]), \"слов\")\n","  symbolsTestText += len(testText[i])\n","  wordsTestText += len(testWordIndexes[i])\n","print('----')\n","print(\"В сумме \", symbolsTestText, \" символов, \", wordsTestText, \" слов\")\n"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"ovkRq0whY40o"},"outputs":[],"source":["def getSetFromIndexes(wordIndexes, xLen, step):\n","  xText = []\n","  wordsLen = len(wordIndexes) # Считаем количество слов\n","  index = 0 # Задаем начальный индекс \n","\n","  while (index + xLen <= wordsLen): # Идём по всей длине вектора индексов\n","    xText.append(wordIndexes[index:index+xLen]) # \"Откусываем\" векторы длины xLen\n","    index += step # Смещаемся вперёд на step\n","    \n","  return xText\n","\n","\n","# Формирование обучающей и проверочной выборки для каждого класса\n","# wordIndexes - массив индексов\n","# xLen - размер окна\n","# step - шаг окна\n","\n","def createSetsMultiClasses(wordIndexes, xLen, step): # Функция принимает последовательность индексов, размер окна, шаг окна\n","  nClasses = len(wordIndexes) # Количество классов\n","  classesXSamples = []        # Здесь будет список размером \"кол-во классов*кол-во окон в тексте*длину окна\"\n","  for wI in wordIndexes:      # Для каждого текста выборки из последовательности индексов\n","    classesXSamples.append(getSetFromIndexes(wI, xLen, step))\n","\n","  # Формируем один общий xSamples\n","  xSamples = []\n","  ySamples = []\n","  \n","  for t in range(nClasses):\n","    xT = classesXSamples[t]\n","    for i in range(len(xT)): # Перебираем каждое окно определенного класса\n","      xSamples.append(xT[i]) # Добавляем в общий список выборки\n","      ySamples.append(utils.to_categorical(t, nClasses)) # Добавляем соответствующий вектор класса\n","\n","  xSamples = np.array(xSamples)\n","  ySamples = np.array(ySamples)\n","\n","  return (xSamples, ySamples)"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"UvgAipZ7fa0I"},"outputs":[],"source":["# ЭТИ ПАРАМЕТРЫ МОЖНО И НУЖНО МЕНЯТЬ\n","# Задаём базовые параметры\n","xLen = 50 # Размер окна (количество слов в векторе)\n","step = 6 # Шаг разбиения текста на векторы"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"Q6ApnH26fcNg"},"outputs":[{"name":"stdout","output_type":"stream","text":["Размерности тренировочного набора\n","(67389, 50)\n","(67389, 2)\n","\n","Размерности тестового набора\n","(868, 50)\n","(868, 2)\n"]}],"source":["xTrainId, yTrain = createSetsMultiClasses(trainWordIndexes, xLen, step)\n","xTestId, yTest = createSetsMultiClasses(testWordIndexes, xLen, xLen)\n","\n","# для каждого \"окна\" класса мы в соответствие ставим значение true или false => поэтому длины x и y одинаковые\n","print(\"Размерности тренировочного набора\")\n","print(xTrainId.shape)\n","print(yTrain.shape)\n","print()\n","print(\"Размерности тестового набора\")\n","print(xTestId.shape)\n","print(yTest.shape)"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[1. 0.]\n"]}],"source":["print(yTrain[0])"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"47lRIem-fuph"},"outputs":[{"name":"stdout","output_type":"stream","text":["[   57     6   387  8522    99 16385   303   642     3     9   564  1405\n","     7   278    17     2  6448   431   295    14   169  2296  8523   126\n","     8     1 10164     1     1     1     3     1     1     6  1607    61\n","  3110 16386  6449     3  5198     2  7377  1067  5783    23   109    60\n","    81   162]\n"]}],"source":["print(xTrainId[0])"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"YHFyIhAHf2I3"},"outputs":[{"name":"stdout","output_type":"stream","text":["Размерность обучающей выборки\n","(67389, 50)\n","[   57     6   387  8522    99 16385   303   642     3     9   564  1405\n","     7   278    17     2  6448   431   295    14   169  2296  8523   126\n","     8     1 10164     1     1     1     3     1     1     6  1607    61\n","  3110 16386  6449     3  5198     2  7377  1067  5783    23   109    60\n","    81   162]\n","\n","Размерность тестовой выборки\n","(868, 50)\n","[  319   699    35     1  4620     5     2   509    22    57   133   171\n","   220 14594     1     1  1667   915     8 13060   100   508  3445  7852\n","    21     1   447    48     1  9296     1    99     1   958  5859    33\n","   388  1082  4865     1  1881     5     8   811  1632   162   462  6219\n","     3     1]\n"]}],"source":["# Для модели BOW - преобразовываем в Bag Of Words\n","xTrainBOW = tokenizer.sequences_to_matrix(xTrainId.tolist())\n","xTestBOW  = tokenizer.sequences_to_matrix(xTestId.tolist())\n","\n","# Для остальных моделей - НЕ преобразовываем в Bag Of Words\n","\n","xTrain = xTrainId\n","xTest = xTestId\n","\n","print(\"Размерность обучающей выборки\")\n","print(xTrain.shape)\n","print(xTrain[0][0:100]) # Фрагмент набора слов \n","\n","print()\n","\n","print(\"Размерность тестовой выборки\")\n","print(xTest.shape)\n","print(xTest[0][0:100]) # Фрагмент набора слов"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[],"source":["np.save('datasets/xTrainBOW.npy', xTrainBOW)\n","np.save('datasets/xTrain.npy', xTrain)\n","np.save('datasets/xTestBOW.npy', xTestBOW)\n","np.save('datasets/xTest.npy', xTest)\n","np.save('datasets/yTrain.npy', yTrain)\n","np.save('datasets/yTest.npy', yTest)"]}],"metadata":{"colab":{"name":"generate_dataset_1.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"}},"nbformat":4,"nbformat_minor":0}
