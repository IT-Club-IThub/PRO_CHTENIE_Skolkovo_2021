{"cells":[{"cell_type":"code","execution_count":11,"metadata":{"id":"QvmWuw8qjYaX"},"outputs":[],"source":["sys.path.append('..')\n","from config import Config\n","\n","import json\n","import numpy as np # Для работы с данными \n","import sys\n","import pymorphy2\n","import os\n","\n","from tensorflow.keras import utils # Для работы с категориальными данными\n","from tensorflow.keras.preprocessing.text import Tokenizer # Методы для работы с текстами"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"NE5t4uWtuIIR"},"outputs":[],"source":["JSON_DATA_DIR = '../../Satellites'"]},{"cell_type":"markdown","metadata":{"id":"yEqPLQ4RWbUE"},"source":["Все тексты true и false объединены в два больших текста;\n","Датасет - список из двух элементов-строк"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":270},"executionInfo":{"elapsed":338,"status":"error","timestamp":1639498780212,"user":{"displayName":"Игорь Молчанов","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjYdLWH6k0781lHDY2fOHciQCMySUAFsaUrdGZFxQ=s64","userId":"05120680139566347989"},"user_tz":-180},"id":"UdQUzDqvvtat","outputId":"dbda315d-a548-4e59-efa6-f4b7def7ff43"},"outputs":[],"source":["# создаём два списка, в пустые элементы которых будем добавлять тексты\n","# в первом элементе будет храниться текст с ответом False, во втором - True\n","textClasses = ['', '']\n","\n","\n","# открываем файл с id эссе и ответами\n","with open(JSON_DATA_DIR + '/train/train_standart.json', 'r') as f_list:\n","  data = json.load(f_list)\n","\n","  # проходимся по каждому \"блоку\" с эссе\n","  for i in range(len(data)):\n","    elem = data[i]\n","\n","    with open(JSON_DATA_DIR + f'/train/essays/{elem[\"id\"]}.json', 'r') as essay:\n","      file = json.load(essay)\n","      text = file['text']\n","      if elem['answer'] == False:\n","        textClasses[0] += text\n","        textClasses[0] += '#'\n","      else:\n","        textClasses[1] += text\n","        textClasses[1] += '#'"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["24526\n","6374\n"]}],"source":["print(len(textClasses[0].split('.')))\n","print(len(textClasses[1].split('.')))"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[],"source":["textClasses[0] = textClasses[0][:len(textClasses[1])]"]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["654756\n","654756\n","е я могу на примере романа И. А. Гончарова «Обломо\n","рии и заключается противоречивость Раскольникова.#\n","222\n","201\n"]}],"source":["print(len(textClasses[0]))\n","print(len(textClasses[1]))\n","\n","print(textClasses[0][-50:])\n","print(textClasses[1][-50:])\n","\n","texts_false = textClasses[0].split(\"#\")\n","texts_true = textClasses[1].split(\"#\")\n","\n","print(len(texts_false))\n","print(len(texts_true))"]},{"cell_type":"code","execution_count":29,"metadata":{},"outputs":[],"source":["test_border_false = len(texts_false)//10\n","test_border_true = len(texts_true)//10\n","\n","val_border_false = len(texts_false)//20 + test_border_false\n","val_border_true = len(texts_true)//20 + test_border_true\n","\n","\n","valText = []\n","valText.append(' '.join(texts_false[:test_border_false]))\n","valText.append(' '.join(texts_true[:test_border_true]))\n","\n","testText = []\n","testText.append(' '.join(texts_false[test_border_false:val_border_false]))\n","testText.append(' '.join(texts_true[test_border_true:val_border_false]))\n","\n","trainText = []\n","trainText.append(' '.join(texts_false[val_border_false:]))\n","trainText.append(' '.join(texts_true[val_border_true:]))"]},{"cell_type":"code","execution_count":30,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["33287\n","40826\n","73617\n","58625\n","547850\n","564659\n"]}],"source":["print(len(testText[0]))\n","print(len(testText[1]))\n","print(len(valText[0]))\n","print(len(valText[1]))\n","print(len(trainText[0]))\n","print(len(trainText[1]))"]},{"cell_type":"markdown","metadata":{},"source":["Список параметров, которые будут меняться в течение генерации датасетов"]},{"cell_type":"code","execution_count":31,"metadata":{},"outputs":[],"source":["numWordsList = [5000, 10000, 20000, \"all\"]\n","pymorphyList = [\"\", \"PyMorphy\"]\n","xLenList = [100, 300, 500]\n","stepList = [\n","    [25,  50,  100],\n","    [100, 150, 300],\n","    [100, 250, 500]\n","]\n","bowList = [\"BOW\", \"\"]"]},{"cell_type":"code","execution_count":32,"metadata":{},"outputs":[],"source":["def getSetFromIndexes(wordIndexes, xLen, step):\n","  xText = []\n","  wordsLen = len(wordIndexes) # Считаем количество слов\n","  index = 0 # Задаем начальный индекс \n","\n","  while (index + xLen <= wordsLen): # Идём по всей длине вектора индексов\n","    xText.append(wordIndexes[index:index+xLen]) # \"Откусываем\" векторы длины xLen\n","    index += step # Смещаемся вперёд на step\n","    \n","  return xText\n","\n","def createSetsMultiClasses(wordIndexes, xLen, step): # Функция принимает последовательность индексов, размер окна, шаг окна\n","  nClasses = len(wordIndexes) # Количество классов\n","  classesXSamples = []        # Здесь будет список размером \"кол-во классов*кол-во окон в тексте*длину окна\"\n","  for wI in wordIndexes:      # Для каждого текста выборки из последовательности индексов\n","    classesXSamples.append(getSetFromIndexes(wI, xLen, step))\n","\n","  # Формируем один общий xSamples\n","  xSamples = []\n","  ySamples = []\n","  \n","  for t in range(nClasses):\n","    xT = classesXSamples[t]\n","    for i in range(len(xT)): # Перебираем каждое окно определенного класса\n","      xSamples.append(xT[i]) # Добавляем в общий список выборки\n","      ySamples.append(utils.to_categorical(t, nClasses)) # Добавляем соответствующий вектор класса\n","\n","  xSamples = np.array(xSamples)\n","  ySamples = np.array(ySamples)\n","\n","  return (xSamples, ySamples)"]},{"cell_type":"code","execution_count":33,"metadata":{},"outputs":[],"source":["def text2Words(text):\n","  morph = pymorphy2.MorphAnalyzer() # Создаем экземпляр класса MorphAnalyzer\n","  words = text.split(' ') # Разделяем текст на пробелы\n","  words = \" \".join([morph.parse(word)[0].normal_form for word in words]) #Переводим каждое слово в нормалную форму  \n","  # words = [morph.parse(word)[0].normal_form for word in words] #Переводим каждое слово в нормалную форму  \n","  return words # Возвращаем слова"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["# tokenizer = Tokenizer(num_words=20000,\n","#     filters='!\"#$%&()*+,-–—./…:;<=>?@[\\\\]^_`{|}~«»\\t\\n\\xa0\\ufeff',\n","#     lower=True,\n","#     split=' ',\n","#     oov_token='unknown',\n","#     char_level=False)\n","\n","# morphAnalyzer = pymorphy2.MorphAnalyzer\n","# nw_tokenizer = morphAnalyzer.iter_known_word_parses(tokenizer)\n","# print(list(nw_tokenizer))"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["# trainText[0] = text2Words(trainText[0])\n","# trainText[1] = text2Words(trainText[1])\n","\n","# tokenizer = Tokenizer(num_words=10000,\n","#                 filters='!\"#$%&()*+,-–—./…:;<=>?@[\\\\]^_`{|}~«»\\t\\n\\xa0\\ufeff',\n","#                 lower=True,\n","#                 split=' ',\n","#                 oov_token='unknown',\n","#                 char_level=False)\n","\n","# tokenizer.fit_on_texts(trainText) # словарь частотности\n","# items = list(tokenizer.word_index.items()) # индексы слов\n","\n","# print(\"Самые часто встречающиеся слова:\")\n","# print(items[:10])\n","# print()\n","# print(\"Самые редко встречающиеся слова:\")\n","# print(items[-10:])\n","# print()\n","# print(\"Размер словаря:\", len(items))"]},{"cell_type":"code","execution_count":34,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","numwords 5000\n","xLenIndex 0\n","step 25\n","bowState BOW\n","bowState \n","step 50\n","bowState BOW\n","bowState \n","step 100\n","bowState BOW\n","bowState \n","xLenIndex 1\n","step 100\n","bowState BOW\n","bowState \n","step 150\n","bowState BOW\n","bowState \n","step 300\n","bowState BOW\n","bowState \n","xLenIndex 2\n","step 100\n","bowState BOW\n","bowState \n","step 250\n","bowState BOW\n","bowState \n","step 500\n","bowState BOW\n","bowState \n","numwords 10000\n","xLenIndex 0\n","step 25\n","bowState BOW\n","bowState \n","step 50\n","bowState BOW\n","bowState \n","step 100\n","bowState BOW\n","bowState \n","xLenIndex 1\n","step 100\n","bowState BOW\n","bowState \n","step 150\n","bowState BOW\n","bowState \n","step 300\n","bowState BOW\n","bowState \n","xLenIndex 2\n","step 100\n","bowState BOW\n","bowState \n","step 250\n","bowState BOW\n","bowState \n","step 500\n","bowState BOW\n","bowState \n","numwords 20000\n","xLenIndex 0\n","step 25\n","bowState BOW\n","bowState \n","step 50\n","bowState BOW\n","bowState \n","step 100\n","bowState BOW\n","bowState \n","xLenIndex 1\n","step 100\n","bowState BOW\n","bowState \n","step 150\n","bowState BOW\n","bowState \n","step 300\n","bowState BOW\n","bowState \n","xLenIndex 2\n","step 100\n","bowState BOW\n","bowState \n","step 250\n","bowState BOW\n","bowState \n","step 500\n","bowState BOW\n","bowState \n","numwords all\n","xLenIndex 0\n","step 25\n","bowState BOW\n","bowState \n","step 50\n","bowState BOW\n","bowState \n","step 100\n","bowState BOW\n","bowState \n","xLenIndex 1\n","step 100\n","bowState BOW\n","bowState \n","step 150\n","bowState BOW\n","bowState \n","step 300\n","bowState BOW\n","bowState \n","xLenIndex 2\n","step 100\n","bowState BOW\n","bowState \n","step 250\n","bowState BOW\n","bowState \n","step 500\n","bowState BOW\n","bowState \n","PyMorphy\n","numwords 5000\n","xLenIndex 0\n","step 25\n","bowState BOW\n","bowState \n","step 50\n","bowState BOW\n","bowState \n","step 100\n","bowState BOW\n","bowState \n","xLenIndex 1\n","step 100\n","bowState BOW\n","bowState \n","step 150\n","bowState BOW\n","bowState \n","step 300\n","bowState BOW\n","bowState \n","xLenIndex 2\n","step 100\n","bowState BOW\n","bowState \n","step 250\n","bowState BOW\n","bowState \n","step 500\n","bowState BOW\n","bowState \n","numwords 10000\n","xLenIndex 0\n","step 25\n","bowState BOW\n","bowState \n","step 50\n","bowState BOW\n","bowState \n","step 100\n","bowState BOW\n","bowState \n","xLenIndex 1\n","step 100\n","bowState BOW\n","bowState \n","step 150\n","bowState BOW\n","bowState \n","step 300\n","bowState BOW\n","bowState \n","xLenIndex 2\n","step 100\n","bowState BOW\n","bowState \n","step 250\n","bowState BOW\n","bowState \n","step 500\n","bowState BOW\n","bowState \n","numwords 20000\n","xLenIndex 0\n","step 25\n","bowState BOW\n","bowState \n","step 50\n","bowState BOW\n","bowState \n","step 100\n","bowState BOW\n","bowState \n","xLenIndex 1\n","step 100\n","bowState BOW\n","bowState \n","step 150\n","bowState BOW\n","bowState \n","step 300\n","bowState BOW\n","bowState \n","xLenIndex 2\n","step 100\n","bowState BOW\n","bowState \n","step 250\n","bowState BOW\n","bowState \n","step 500\n","bowState BOW\n","bowState \n","numwords all\n","xLenIndex 0\n","step 25\n","bowState BOW\n","bowState \n","step 50\n","bowState BOW\n","bowState \n","step 100\n","bowState BOW\n","bowState \n","xLenIndex 1\n","step 100\n","bowState BOW\n","bowState \n","step 150\n","bowState BOW\n","bowState \n","step 300\n","bowState BOW\n","bowState \n","xLenIndex 2\n","step 100\n","bowState BOW\n","bowState \n","step 250\n","bowState BOW\n","bowState \n","step 500\n","bowState BOW\n","bowState \n"]}],"source":["labels = ['False', 'True']\n","labelsNum = len(labels)\n","\n","for pymorphyState in pymorphyList:\n","    print(pymorphyState)\n","    \n","    if pymorphyState:\n","        for i in range(2):\n","            trainText[i] = text2Words(trainText[i])\n","            valText[i] = text2Words(valText[i])\n","            testText[i] = text2Words(testText[i])\n","        \n","    for numWords in numWordsList:\n","        print(\"numwords\", numWords)\n","        tokenizer = Tokenizer(num_words=numWords,\n","            filters='!\"#$%&()*+,-–—./…:;<=>?@[\\\\]^_`{|}~«»\\t\\n\\xa0\\ufeff',\n","            lower=True,\n","            split=' ',\n","            oov_token='unknown',\n","            char_level=False)\n","            \n","        if numWords == \"all\":\n","            tokenizer = Tokenizer(\n","                filters='!\"#$%&()*+,-–—./…:;<=>?@[\\\\]^_`{|}~«»\\t\\n\\xa0\\ufeff',\n","                lower=True,\n","                split=' ',\n","                oov_token='unknown',\n","                char_level=False)\n","            \n","            tokenizer.fit_on_texts(trainText)\n","            items = list(tokenizer.word_index.items())\n","            numWords = len(items)\n","            \n","        tokenizer.fit_on_texts(trainText)\n","\n","        trainWordIndexes = tokenizer.texts_to_sequences(trainText)\n","        valWordIndexes = tokenizer.texts_to_sequences(valText)\n","        testWordIndexes = tokenizer.texts_to_sequences(testText)\n","\n","        for xLenIndex in range(len(xLenList)):\n","            print(\"xLenIndex\", xLenIndex)\n","            for step in stepList[xLenIndex]:\n","                print(\"step\", step)\n","                xLen = xLenList[xLenIndex]\n","                step = step\n","\n","                xTrainId, yTrain = createSetsMultiClasses(trainWordIndexes, xLen, step)\n","                xValId, yVal = createSetsMultiClasses(valWordIndexes, xLen, step)\n","                xTestId, yTest = createSetsMultiClasses(testWordIndexes, xLen, step)\n","\n","                for bowState in bowList:\n","                    print(\"bowState\", bowState)\n","                    if bowState:\n","                        xTrain = tokenizer.sequences_to_matrix(xTrainId.tolist())\n","                        xVal = tokenizer.sequences_to_matrix(xValId.tolist())\n","                        xTest  = tokenizer.sequences_to_matrix(xTestId.tolist())\n","                    else:\n","                        xTrain = xTrainId\n","                        xVal = xValId\n","                        xTest = xTestId\n","\n","                    np.savez_compressed(f\"{Config.DATASETS_DIR}dataset_numWords_{numWords}_{pymorphyState}_xLen_{xLen}_step_{step}_{bowState}.npz\", xTrain=xTrain, yTrain=yTrain, xVal=xVal, yVal=yVal, xTest=xTest, yTest=yTest)\n","            "]}],"metadata":{"colab":{"name":"generate_dataset_1.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"}},"nbformat":4,"nbformat_minor":0}
