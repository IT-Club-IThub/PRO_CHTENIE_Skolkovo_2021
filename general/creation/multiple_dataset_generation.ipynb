{"cells":[{"cell_type":"code","execution_count":2,"metadata":{"id":"QvmWuw8qjYaX"},"outputs":[],"source":["sys.path.append('..')\n","from config import Config\n","\n","import json\n","import numpy as np # Для работы с данными \n","import sys\n","import os\n","\n","from tensorflow.keras import utils # Для работы с категориальными данными\n","from tensorflow.keras.preprocessing.text import Tokenizer # Методы для работы с текстами"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"NE5t4uWtuIIR"},"outputs":[],"source":["JSON_DATA_DIR = '../../Satellites'"]},{"cell_type":"markdown","metadata":{"id":"yEqPLQ4RWbUE"},"source":["Все тексты true и false объединены в два больших текста;\n","Датасет - список из двух элементов-строк"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":270},"executionInfo":{"elapsed":338,"status":"error","timestamp":1639498780212,"user":{"displayName":"Игорь Молчанов","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjYdLWH6k0781lHDY2fOHciQCMySUAFsaUrdGZFxQ=s64","userId":"05120680139566347989"},"user_tz":-180},"id":"UdQUzDqvvtat","outputId":"dbda315d-a548-4e59-efa6-f4b7def7ff43"},"outputs":[],"source":["# создаём два списка, в пустые элементы которых будем добавлять тексты\n","# в первом элементе будет храниться текст с ответом False, во втором - True\n","textClasses = ['', '']\n","\n","\n","# открываем файл с id эссе и ответами\n","with open(JSON_DATA_DIR + '/train/train_standart.json', 'r') as f_list:\n","  data = json.load(f_list)\n","\n","  # проходимся по каждому \"блоку\" с эссе\n","  for i in range(len(data)):\n","    elem = data[i]\n","\n","    with open(JSON_DATA_DIR + f'/train/essays/{elem[\"id\"]}.json', 'r') as essay:\n","      file = json.load(essay)\n","      text = file['text']\n","      if elem['answer'] == False:\n","        textClasses[0] += text\n","        textClasses[0] += '#'\n","      else:\n","        textClasses[1] += text\n","        textClasses[1] += '#'"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["24526\n","6374\n"]}],"source":["print(len(textClasses[0].split('.')))\n","print(len(textClasses[1].split('.')))"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["2587553\n","654756\n","вторитет СССР и провести политику десталинизации.#\n","рии и заключается противоречивость Раскольникова.#\n","801\n","201\n","Считаю, почти все дадут согласие, что в протяжении жизни мы ни один раз встречаем безответственных профессионалов собственного дела. К несчастью, если дело касается медицины, то безалаберное отношение может сыграть злую шуточку. Над неувязкой ответственности анализирует автор текста. Василий Павлович Аксенов думает, что к собственной работе нужно относится серьезно и радиво, в особенности если идет речь о здоровье остальных людей. Свое мнение автор обосновывает на примере ситуации из текста.\n","Василий Павлович Аксенов думает, что к собственной работе нужно относится серьезно и радиво, в особенности если идет речь о здоровье остальных людей. Свое мнение автор обосновывает на примере ситуации из текста. Василий Павлович противопоставляет 2-ух мед сотрудников различных поколений. Юный спец Александр Зеленин чрезвычайно трепетно подступает к реализации собственных обязательств. Он не может «закрыть глаза» на почти все упущения. Автор подчеркивает это в предложениях 2,3. Не считая того свежеиспеченный руководитель не лишь проходится по поликлинике, да и изучает старые истории заболеваний. Это подтверждается в предложениях 11-12. В целях изменений ответственный Зеленин собирает производственное заседание. Юному спецу автор противопоставляет медработника Макара Ивановича со стажем работы больше 30 лет. Скорее всего, мужчине уже надоела своя работа и он стал безответственно подступать к её реализации. Автор подчеркивает это в предложениях 9-12, 18. Не считая того Макар Иванович не ощущает, что делает что-то не так, он не желает признавать собственных просчетов и поменять подход к исцелению пациентов. Из предложений 36-47 мы можем прийти к выводу, что медработник не хочет никаких изменений и, при этом, не намерена прислушиваться к словам юного руководителя. Позиция автора ясна. К хоть какой работе нужно относится радиво, в особенности если от нее зависит здоровье остальных людей. В наше время повсевременно что-то изменяется, возникает. Конкретно потому нельзя отставать от развития. Я всецело согласна с позицией автора. Считаю, во всегда были люди, безответственно пригодные к собственной работе. К примеру, в романе Ивана Александровича Гончарова «Обломов» основной герой Илья Ильич привык к бесцельному стилю жизни, он запамятывает об управлении своим имением и всецело теряет связь с работой. Не умопомрачительно, что с каждым годом жизнь в Обломовке становится все ужаснее и ужаснее. В итоге, подводя результаты произнесенному, можно прийти к выводу. Любой из нас должен с совестью подступать к выполнению собственных должностных обязательств. Нужно осознавать, что все взаимосвязано, и от нашей ответственности зависит благополучие остальных людей.\n"]}],"source":["print(len(textClasses[0]))\n","print(len(textClasses[1]))\n","\n","print(textClasses[0][-50:])\n","print(textClasses[1][-50:])\n","\n","texts_false = textClasses[0].split(\"#\")\n","texts_true = textClasses[1].split(\"#\")\n","\n","print(len(texts_false))\n","print(len(texts_true))\n","\n","print(texts_false[0])"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["test_border_false = len(texts_false)//10\n","test_border_true = len(texts_true)//10\n","\n","val_border_false = len(texts_false)//20 + test_border_false\n","val_border_true = len(texts_true)//20 + test_border_true\n","\n","\n","valText = []\n","valText.append(' '.join(texts_false[:test_border_false]))\n","valText.append(' '.join(texts_true[:test_border_true]))\n","\n","testText = []\n","testText.append(' '.join(texts_false[test_border_false:val_border_false]))\n","testText.append(' '.join(texts_true[test_border_true:val_border_true]))\n","\n","trainText = []\n","trainText.append(' '.join(texts_false[val_border_false:]))\n","trainText.append(' '.join(texts_true[val_border_true:]))"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["1276\n","309\n","2532\n","655\n","20720\n","5412\n"]}],"source":["print(len(testText[0].split(\".\")))\n","print(len(testText[1].split(\".\")))\n","print(len(valText[0].split(\".\")))\n","print(len(valText[1].split(\".\")))\n","print(len(trainText[0].split(\".\")))\n","print(len(trainText[1].split(\".\")))"]},{"cell_type":"markdown","metadata":{},"source":["Список параметров, которые будут меняться в течение генерации датасетов"]},{"cell_type":"code","execution_count":28,"metadata":{},"outputs":[],"source":["numWordsList = [10000, 20000, 30000, \"all\"]\n","xLenList = [100, 300, 500]\n","stepList = [\n","    [(25, 6), (50, 12), (75, 17)],\n","    [(100, 25), (150, 35), (250, 60)],\n","    [(150, 35), (250, 60), (350, 85)]\n","]\n","bowList = [\"BOW\", \"\"]"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["def getSetFromIndexes(wordIndexes, xLen, step):\n","  xText = []\n","  wordsLen = len(wordIndexes) # Считаем количество слов\n","  index = 0 # Задаем начальный индекс \n","\n","  while (index + xLen <= wordsLen): # Идём по всей длине вектора индексов\n","    xText.append(wordIndexes[index:index+xLen]) # \"Откусываем\" векторы длины xLen\n","    index += step # Смещаемся вперёд на step\n","    \n","  return xText\n","\n","def createSetsMultiClasses(wordIndexes, xLen, step): # Функция принимает последовательность индексов, размер окна, шаг окна\n","  nClasses = len(wordIndexes) # Количество классов\n","  classesXSamples = []        # Здесь будет список размером \"кол-во классов*кол-во окон в тексте*длину окна\"\n","  for wIclass in range(nClasses):      # Для каждого текста выборки из последовательности индексов\n","    if wIclass == 0:\n","      classesXSamples.append(getSetFromIndexes(wordIndexes[0], xLen, step[0]))\n","    else:\n","      classesXSamples.append(getSetFromIndexes(wordIndexes[1], xLen, step[1]))\n","\n","  # Формируем один общий xSamples\n","  xSamples = []\n","  ySamples = []\n","  \n","  for t in range(nClasses):\n","    xT = classesXSamples[t]\n","    for i in range(len(xT)): # Перебираем каждое окно определенного класса\n","      xSamples.append(xT[i]) # Добавляем в общий список выборки\n","      ySamples.append(utils.to_categorical(t, nClasses)) # Добавляем соответствующий вектор класса\n","\n","  xSamples = np.array(xSamples)\n","  ySamples = np.array(ySamples)\n","\n","  return (xSamples, ySamples)"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["# tokenizer = Tokenizer(num_words=20000,\n","#     filters='!\"#$%&()*+,-–—./…:;<=>?@[\\\\]^_`{|}~«»\\t\\n\\xa0\\ufeff',\n","#     lower=True,\n","#     split=' ',\n","#     oov_token='unknown',\n","#     char_level=False)\n","\n","# morphAnalyzer = pymorphy2.MorphAnalyzer\n","# nw_tokenizer = morphAnalyzer.iter_known_word_parses(tokenizer)\n","# print(list(nw_tokenizer))"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["# trainText[0] = text2Words(trainText[0])\n","# trainText[1] = text2Words(trainText[1])\n","\n","# tokenizer = Tokenizer(num_words=10000,\n","#                 filters='!\"#$%&()*+,-–—./…:;<=>?@[\\\\]^_`{|}~«»\\t\\n\\xa0\\ufeff',\n","#                 lower=True,\n","#                 split=' ',\n","#                 oov_token='unknown',\n","#                 char_level=False)\n","\n","# tokenizer.fit_on_texts(trainText) # словарь частотности\n","# items = list(tokenizer.word_index.items()) # индексы слов\n","\n","# print(\"Самые часто встречающиеся слова:\")\n","# print(items[:10])\n","# print()\n","# print(\"Самые редко встречающиеся слова:\")\n","# print(items[-10:])\n","# print()\n","# print(\"Размер словаря:\", len(items))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["labels = ['False', 'True']\n","labelsNum = len(labels)\n","\n","        \n","for numWords in numWordsList:\n","    print(\"numwords\", numWords)\n","    tokenizer = Tokenizer(num_words=numWords,\n","        filters='!\"#$%&()*+,-–—./…:;<=>?@[\\\\]^_`{|}~«»\\t\\n\\xa0\\ufeff',\n","        lower=True,\n","        split=' ',\n","        oov_token='unknown',\n","        char_level=False)\n","        \n","    if numWords == \"all\":\n","        tokenizer = Tokenizer(\n","            filters='!\"#$%&()*+,-–—./…:;<=>?@[\\\\]^_`{|}~«»\\t\\n\\xa0\\ufeff',\n","            lower=True,\n","            split=' ',\n","            oov_token='unknown',\n","            char_level=False)\n","        \n","        tokenizer.fit_on_texts(trainText)\n","        items = list(tokenizer.word_index.items())\n","        numWords = len(items) - 1\n","        \n","    tokenizer.fit_on_texts(trainText)\n","\n","    trainWordIndexes = tokenizer.texts_to_sequences(trainText)\n","    valWordIndexes = tokenizer.texts_to_sequences(valText)\n","    testWordIndexes = tokenizer.texts_to_sequences(testText)\n","\n","    for xLenIndex in range(len(xLenList)):\n","        print(\"xLen\", xLenList[xLenIndex])\n","        for step in stepList[xLenIndex]:\n","            print(\"step\", step)\n","            xLen = xLenList[xLenIndex]\n","\n","            xTrainId, yTrain = createSetsMultiClasses(trainWordIndexes, xLen, step)\n","            xValId, yVal = createSetsMultiClasses(valWordIndexes, xLen, step)\n","            xTestId, yTest = createSetsMultiClasses(testWordIndexes, xLen, xLen)\n","\n","            for bowState in bowList:\n","                print(\"bowState\", bowState)\n","                if bowState:\n","                    xTrain = tokenizer.sequences_to_matrix(xTrainId.tolist())\n","                    xVal = tokenizer.sequences_to_matrix(xValId.tolist())\n","                    xTest  = tokenizer.sequences_to_matrix(xTestId.tolist())\n","                else:\n","                    xTrain = xTrainId\n","                    xVal = xValId\n","                    xTest = xTestId\n","\n","                np.savez_compressed(f\"{Config.DATASETS_DIR}dataset_numWords_{numWords}_xLen_{xLen}_step_{step}_{bowState}.npz\", xTrain=xTrain, yTrain=yTrain, xVal=xVal, yVal=yVal, xTest=xTest, yTest=yTest)\n","        "]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["def getSetFromIndexes(wordIndexes, xLen, step):\n","  xText = []\n","  wordsLen = len(wordIndexes) # Считаем количество слов\n","  index = 0 # Задаем начальный индекс \n","\n","  while (index + xLen <= wordsLen): # Идём по всей длине вектора индексов\n","    xText.append(wordIndexes[index:index+xLen]) # \"Откусываем\" векторы длины xLen\n","    index += step # Смещаемся вперёд на step\n","    \n","  return xText\n","\n","def createSetsMultiClassesTest(wordIndexes, xLen, step): # Функция принимает последовательность индексов, размер окна, шаг окна\n","  nClasses = len(wordIndexes) # Количество классов\n","  classesXSamples = []        # Здесь будет список размером \"кол-во классов*кол-во окон в тексте*длину окна\"\n","  for wI in wordIndexes:      # Для каждого текста выборки из последовательности индексов\n","    classesXSamples.append(getSetFromIndexes(wordIndexes[0], xLen, step))\n","\n","\n","  # Формируем один общий xSamples\n","  xSamples = []\n","  ySamples = []\n","  \n","  for t in range(nClasses):\n","    xT = classesXSamples[t]\n","    for i in range(len(xT)): # Перебираем каждое окно определенного класса\n","      xSamples.append(xT[i]) # Добавляем в общий список выборки\n","      ySamples.append(utils.to_categorical(t, nClasses)) # Добавляем соответствующий вектор класса\n","\n","  xSamples = np.array(xSamples)\n","  ySamples = np.array(ySamples)\n","\n","  return (xSamples, ySamples)"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"ename":"NameError","evalue":"name 'numWordsList' is not defined","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[1;32mC:\\Users\\42E9~1\\AppData\\Local\\Temp/ipykernel_32724/3667454460.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mnumWords\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnumWordsList\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"numwords\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumWords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     tokenizer = Tokenizer(num_words=numWords,\n","\u001b[1;31mNameError\u001b[0m: name 'numWordsList' is not defined"]}],"source":["labels = ['False', 'True']\n","labelsNum = len(labels)\n","\n","        \n","for numWords in numWordsList:\n","    print(\"numwords\", numWords)\n","    tokenizer = Tokenizer(num_words=numWords,\n","        filters='!\"#$%&()*+,-–—./…:;<=>?@[\\\\]^_`{|}~«»\\t\\n\\xa0\\ufeff',\n","        lower=True,\n","        split=' ',\n","        oov_token='unknown',\n","        char_level=False)\n","        \n","    if numWords == \"all\":\n","        tokenizer = Tokenizer(\n","            filters='!\"#$%&()*+,-–—./…:;<=>?@[\\\\]^_`{|}~«»\\t\\n\\xa0\\ufeff',\n","            lower=True,\n","            split=' ',\n","            oov_token='unknown',\n","            char_level=False)\n","        \n","        tokenizer.fit_on_texts(trainText)\n","        items = list(tokenizer.word_index.items())\n","        numWords = len(items) - 1\n","        \n","    tokenizer.fit_on_texts(trainText)\n","\n","    trainWordIndexes = tokenizer.texts_to_sequences(trainText)\n","    valWordIndexes = tokenizer.texts_to_sequences(valText)\n","\n","    for xLenIndex in range(len(xLenList)):\n","        print(\"xLen\", xLenList[xLenIndex])\n","        for step in stepList[xLenIndex]:\n","            print(\"step\", step)\n","            xLen = xLenList[xLenIndex]\n","\n","            xTrainId, yTrain = createSetsMultiClasses(trainWordIndexes, xLen, step)\n","            xValId, yVal = createSetsMultiClasses(valWordIndexes, xLen, step)\n","\n","            for bowState in bowList:\n","                print(\"bowState\", bowState)\n","                if bowState:\n","                    xTrain = tokenizer.sequences_to_matrix(xTrainId.tolist())\n","                    xVal = tokenizer.sequences_to_matrix(xValId.tolist())\n","                else:\n","                    xTrain = xTrainId\n","                    xVal = xValId\n","\n","                np.savez_compressed(f\"{Config.DATASETS_DIR}dataset_numWords_{numWords}_xLen_{xLen}_step_{step}_{bowState}.npz\", xTrain=xTrain, yTrain=yTrain, xVal=xVal, yVal=yVal, xTest=xTest, yTest=yTest)"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[],"source":["   \n","tokenizer = Tokenizer(\n","    filters='!\"#$%&()*+,-–—./…:;<=>?@[\\\\]^_`{|}~«»\\t\\n\\xa0\\ufeff',\n","    lower=True,\n","    split=' ',\n","    oov_token='unknown',\n","    char_level=False)\n","\n","tokenizer.fit_on_texts(trainText)\n","# items = list(tokenizer.word_index.items())\n","numWords = 48131\n","\n","trainWordIndexes = tokenizer.texts_to_sequences(trainText)\n","valWordIndexes = tokenizer.texts_to_sequences(valText)\n","testWordIndexes = tokenizer.texts_to_sequences(testText)\n","\n","xLen = 300\n","step = (150, 35)\n","\n","xTrainId, yTrain = createSetsMultiClasses(trainWordIndexes, xLen, step)\n","xValId, yVal = createSetsMultiClasses(valWordIndexes, xLen, step)\n","xTestId, yTest = createSetsMultiClassesTest(testWordIndexes, xLen, xLen)\n","\n","xTrain = xTrainId\n","xVal = xValId\n","xTest  = xTestId\n","np.savez_compressed(f\"{Config.DATASETS_DIR}NEWdataset_numWords_{numWords}_xLen_{xLen}_step_{xLen}_.npz\", xTrain=xTrain, yTrain=yTrain, xVal=xVal, yVal=yVal, xTest=xTest, yTest=yTest)\n"]}],"metadata":{"colab":{"name":"generate_dataset_1.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"}},"nbformat":4,"nbformat_minor":0}
